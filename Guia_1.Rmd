---
title: "Ejercicios Regresión"
author: "Gloria"
date: '`r Sys.Date()`'
output: 
  
  html_document:
    keep_md: true
    toc: true
    toc_depth: 4
    code_folding: show
    toc_float: yes
    df_print: paged
    theme: flatly
    code_download: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,  echo = FALSE, warning= FALSE )
```
<br>
<br>

# Capítulo 1. Regresión Lineal Simple


<br>

## <span style="color:#18bc9c">Correlación</span>

<br>

### Ejercicio 1.1. \
<br>
En el archivo grasacerdos.xlsx se encuentran los datos del peso
vivo (PV, en Kg) y al espesor de grasa dorsal (EGD, en mm) de 30 lechones
elegidos al azar de una población de porcinos Duroc Jersey del Oeste de la
provincia de Buenos Aires. Se pide: \
<br>
(a) Dibujar el diagrama de dispersión e interpretarlo.\
<br>
(b) Calcular el coeficiente de correlación muestral y explíquelo.\
<br>
(c) ¿Hay suficiente evidencia para admitir asociación entre el peso y el espesor de grasa? $(\alpha = 0,05)$. Verifique los supuestos para decidir el indicador
que va a utilizar.\


```{r}

library(readxl)
library(ggplot2)

grasacerdos <- read_excel("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/grasacerdos.xlsx")
print(head(grasacerdos,5))

grasacerdos[] <- lapply(grasacerdos, function(x) {
  if (is.character(x)) gsub(",", ".", x) else x
})


```
Cambiamos las "," por "." y visualizamos cuantos nulos hay  por columna. Convertimos las columnas de PV y EGD a numéricas. Pedimos el summary del dataframe. 

```{r}
print(str(grasacerdos))

print(sapply(grasacerdos, function(x) sum(is.na(x)))) #cuantos valores nulos por columna tiene el dataframe

library(dplyr)
grasacerdos <- grasacerdos|>transmute(
  PV= as.numeric(PV),
  EGD= as.numeric(EGD)
)
summary(grasacerdos)

```
<br>

#### (a) 


```{r}
library(lmtest)
attach(grasacerdos)

plot( PV,EGD, xlab= "PV, en Kg", ylab= "EGD, en mm", main= "Peso vivo vs Espesor de grasa dorsal", col= "#2c3e50", pch= 19)


```
```{r}
#Diagrama con ggplot
ggplot(data = grasacerdos, aes(x = PV, y = EGD)) +
  geom_point() +
  ggtitle("Peso vs  Espesor de grasa dorsal") +
  theme_bw() + theme(plot.title = element_text(hjust = 0.5))
```
En los datos visualizados  no pareciera haber asociación entre las variables. 

#### (b)
<br>
En primer lugar analizo la normalidad de las variables mediante los gráficos de los histogramas, los qqplots y la prueba de Shapiro.
```{r}
par(mfrow = c(1, 2)) 
hist(PV, breaks = 10, main = "", xlab = "Peso VIVO", border = "#2c3e50") 
hist(EGD, breaks = 10, main = "", xlab = "Espesor de grasa dorsal", border = "#18bc9c")
```
```{r}
par(mfrow = c(1, 2)) 
qqnorm(PV, main = "Peso vivo", col = "#2c3e50") 
qqline(PV) 
qqnorm(EGD, main = "Espesor de grasa dorsal", col = "#18bc9c") 
qqline(EGD)
```
```{r}
par(mfrow = c(1, 1)) 

#Test de hipótesis para el análisis de normalidad 
shapiro.test(PV)
```


```{r}
shapiro.test(EGD)
```
Por el test de Shapiro Wilk no se puede rechazar la normalidad de los datos en ninguno de los dos casos. 
<br>
Análisis de la normalidad multivariada - Test de Henze Zirkler

```{r, echo=TRUE}
#Análisis de normalidad bivariada 
library(MVN)
attach(grasacerdos)
peso_egd=data.frame(PV,EGD)
#Usamos Test Henze-Zirkler para evaluar normalidad multivariada (bivariada en este caso)
respuesta_testHZ<-mvn(peso_egd , mvnTest = "hz")
print(respuesta_testHZ$multivariateNormality)
```
El test da por resultado que las variables son normales bivariadas.

La correlación entre ambas variables es:

```{r}
cor(PV, EGD, method= "pearson")
```
La correlación entre las variables es baja: Si el valor de r es cercano a 0, indica que no existe una tendencia
creciente o decreciente entre las variables estudiadas.


```{r}
cor.test(PV, EGD,method="pearson")
```
El test arroja que el valor p de la prueba de Pearson es 0.175. 


```{r}
library(corrplot)
M= cor(grasacerdos)
M
```
Si bien no es necesario aplicar el coeficiente de Spearman pues se satisfacen los supuestos, igualmente lo hago.

```{r}
cor.test(PV, EGD,method="spearman")
```

### Ejercicio 1.2.
<br>

Los datos del cuarteto de Anscombe se encuentran en el archivo
 anscombe.xlsx
 Se pide explorar los datos de la siguiente manera:\
 <br>
 (a) Graficar los cuatro pares de datos en un diagrama de dispersión cada
 uno.\
 <br>
 (b) Hallar los valores medios de las variables para cada para de datos.\
<br> (c) Hallar los valores de la dispersión para cada conjunto de datos.\
 <br>(d) Hallar el coeficiente muestral de correlación lineal en cada caso.\
 <br>(e) Observar, comentar y concluir.\
 <br>
 
#### (a)


```{r}
library(readxl)
library(gt)

anscombe_completos <- read_excel("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/anscombe_completos.xlsx", 
    col_types = c("numeric", "numeric", "numeric", 
        "numeric", "numeric", "numeric", 
        "numeric", "numeric"))

anscombe_completos[2,2]<-6.95



get_stats <- function(x, y) {
  # Create and store the linear model
  lin_mod <- lm(y ~ x)
  
  # Create data frame with statistics
  data.frame(
    media_x = mean(x),
    media_y = mean(y),
    var_x = var(x),
    var_y = var(y),
    correl = cor(x, y),
    rcuad = summary(lin_mod)$r.squared  # Fixed this line
  )
}


```


```{r}

resultados <- data.frame()

for (i in 1:4) {
  x <- anscombe_completos[[paste0("x", i)]]
  y <- anscombe_completos[[paste0("y", i)]]
  stats <- get_stats(x, y)
  stats$Grupo <- paste0("Grupo ", i)
  resultados <- rbind(resultados, stats)
  
}


```


```{r}
library(ggplot2)

# Crear un DataFrame combinando los datos de los cuatro grupos
anscombe_long <- data.frame(
  x = unlist(lapply(1:4, function(i) anscombe_completos[[paste0("x", i)]])),
  y = unlist(lapply(1:4, function(i) anscombe_completos[[paste0("y", i)]])),
  grupo = rep(paste0("Grupo ", 1:4), each = nrow(anscombe_completos))
)

if (!requireNamespace("ggthemes", quietly = TRUE)) {
  install.packages("ggthemes")
}

# Cargar ggthemes
library(ggthemes)

# Graficar los datos con ggplot2 y aplicar el tema Flatly
ggplot(anscombe_long, aes(x = x, y = y)) +
  geom_point(color="#2c3e50") +
  facet_wrap(~ grupo) +
  labs(title = "Gráficos de los cuatro grupos de Anscombe",
       x = "X",
       y = "Y") +
  theme_fivethirtyeight() +  # Tema Flatly de ggthemes
  theme(axis.title = element_text())
```

#### (b)

```{r}
resultados <- resultados |> select(Grupo, media_x, media_y, var_x, var_y, correl, rcuad) |> gt()

resultados
```  
## <span style="color:#18bc9c"> Modelo Lineal Simple</span>
<br>

### Ejercicio 1.3. \
<br>
El archivo peso_edad_colest.xlsx disponible en contiene regis
tros correspondientes a 25 individuos respecto de su peso, su edad y el nivel
 de colesterol total en sangre.
 Se pide: \
 <br>
 (a) Realizar el diagrama de dispersión de colesterol en función de la edad y
 de colesterol en función de peso. Le parece adecuado ajustar un modelo
 lineal para alguno de estos dos pares de variables?\
  <br>
 (b) Estime los coeficientes del modelo lineal para el colesterol en función de
 la edad.\
  <br>
 (c) Estime intervalos de confianza del 95% para los coeficientes del modelo
 y compare estos resultados con el test de Wald para los coeficientes. Le
 parece que hay asociación entre estos test y el test de la regresión?\
  <br>
 (d) A partir de esta recta estime los valores de E(Y) para x = 25 años y
 x =48años. Podría estimarse el valor de E(Y) para x = 80 años? \
  <br>
 (e) Testee la normalidad de los residuos y haga un gráfico para ver si son
 homocedásticos.\
  <br>

#### (a)

```{r}
library(readxl)
peso_edad_colest <- read_excel("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/peso_edad_colest.xlsx")

library(ggplot2)
library(tidyverse)
library(patchwork)

p1<- ggplot(peso_edad_colest, aes(x = edad, y = colest)) +
  geom_point() +
  labs(title = "Colesterol vs Edad", x = "Edad", y = "Colesterol") +
  theme_minimal()
p2 <- ggplot(peso_edad_colest, aes(x = peso, y = colest)) +
  geom_point() +
  labs(title = "Colesterol vs Peso", x = "Peso", y = "Colesterol") +
  theme_minimal()

p1 / p2 

```
Viendo los gráficos de dispersión, pareciera  que el colesterol en función de la edad tiene una relación lineal más clara que el colesterol en función del peso. Por lo tanto, es más adecuado ajustar un modelo lineal para el colesterol en función de la edad.  

#### (b)

```{r}
modelo_edad <- lm(colest ~ edad, data =peso_edad_colest)

print(summary(modelo_edad))

print(anova(modelo_edad))
```


```{r}
sd(peso_edad_colest$colest)*0.8811468/sd(peso_edad_colest$edad)
```


```{r}
cor(peso_edad_colest$colest, peso_edad_colest$edad, method = "pearson")
```

#### (c)

Test de Wald para los coeficientes del modelo:

```{r, echo=TRUE}
library(aod)
wald.test(b = coef(modelo_edad), Sigma = vcov(modelo_edad), Terms = 1:2) # chequeo B0 y B1
```
La prueba arroja que el valor p para la prueba de Wald es menor a 0.05, por lo que se rechaza la hipótesis nula de que los coeficientes son iguales a cero. Esto indica que hay evidencia para suponer una asociación significativa entre la edad y el colesterol total en sangre.\
<br>
Los intervalos de confianza a nivel 95% para los coeficientes del modelo son:

```{r, echo= TRUE}
confint(modelo_edad, level = 0.95)
```
Tanto los resultados del test de Wald como los intervalos de confianza coinciden que los coeficientes de la regresión son diferentes a cero. 

#### (d)


 
 
```{r}
# Estimación de E(Y) para x = 25 y x = 48

Estimacion_25= coef(modelo_edad)[1]+coef(modelo_edad)[2]*25
print (paste("La estimación para x=25 es", Estimacion_25) )
Estimacion_48= coef(modelo_edad)[1]+coef(modelo_edad)[2]*48
print (paste("La estimación para x= 48 es ", Estimacion_48))
```
```{r, echo=TRUE}
# Estimación de E(Y) para x = 80
 print(max(peso_edad_colest$edad))
 print(min(peso_edad_colest$edad))

```
Como los datos de edad que tenemos son toma los valores entre 20 y 60, no es posible realizar una estimación para X=80.

#### (e)  

Análisis de la normalidad de los residuos del modelo: \
<br>

```{r}
edad <- peso_edad_colest
edad$prediccion <- modelo_edad$fitted.values 
edad$residuos <- modelo_edad$residuals

ggplot(data = edad, aes(x = residuos)) + geom_histogram(aes(y = ..density..)) + 
  labs(title = "Histograma de los residuos") + theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
qqnorm(modelo_edad$residuals) 
qqline(modelo_edad$residuals)
```
```{r}
shapiro.test(modelo_edad$residuals)
```
  
  
El p-valor de la prueba de Shapiro-Wilk es 0.5175, lo que indica que no se puede rechazar la hipótesis nula de normalidad de los residuos. \
<br>
Análisis de la homocedasticidad de los residuos del modelo: \

```{r}
ggplot(data = edad, aes(x = prediccion, y = residuos)) + 
  geom_point(aes(color = residuos)) + 
  scale_color_gradient2(low = "blue3", mid = "grey", high = "red") + 
  geom_hline(yintercept = 0) + geom_segment(aes(xend = prediccion, yend = 0), alpha = 0.2) + 
  labs(title = "Distribución de los residuos", x = "predicción modelo", y = "residuo") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")

```
No se observa estructura en los residuos del modelo. \
<br>
Test de Breusch-Pagan para la homocedasticidad: \
```{r, echo=TRUE}
library(lmtest)
bptest(modelo_edad)
```
El p-value de la Prueba de Breusch-PAgan es 0.6908, por lo que no se rechaza la hipótesis nula de homocedasticidad. \
<br>
Gráfico de análisis de independencia de las observaciones.\

```{r}
ggplot(data = edad, aes(x = seq_along(residuos), y = residuos)) + 
  geom_point(aes(color = residuos)) + 
  scale_color_gradient2(low = "blue3", mid = "grey", high = "red") + 
  geom_line(size = 0.3) + labs(title = "Distribución de los residuos", x = "index", y = "residuo")+ 
  geom_hline(yintercept = 0) + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")
```
No se detecta ningún patrón en el gráfico de los residuos.\
<br>
Test de Durbin-Watson para analizar la autocorrelación de los residuos:

```{r, echo=TRUE}

library(car)
dwtest(modelo_edad)
```
No se rechaza la  NO AUTOCORRELACIÓN de los residuos. Se valida el supuesto de independencia de las observaciones.\

<br>

## <span style="color:#18bc9c">Transformación de Variables</span>

<br>

### Ejercicio 1.4.\ PREGUNTAR
<br>
Una empresa desarrolló un sistema de energía solar para calentar el agua para una caldera que es parte del sistema de energía del proceso
productivo. Existe el interés de controlar la estabilidad del sistema, para ello
se monitorea el mismo y se registran los datos cada hora. Los datos se encuentran disponibles en el archivo energia.xlsx.\
<br>
(a) Realizar el diagrama de dispersión y evaluar si un modelo de regresión
lineal es adecuado.\
<br>
(b) Estimar un modelo lineal y verificar la normalidad de los residuos del
mismo.\
<br>
(c) En caso de rechazar este supuesto buscar una transformación lineal para
este modelo y aplicarla.\
<br>
(d) Realizar el análisis diagnóstico del nuevo modelo y estimar un intervalo
de confianza y un intervalo de predicción para 27.5 hs con ambos modelos. Comparar los intervalos.\
<br>

```{r}
energia <- read_excel("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/energia.xlsx")
summary(energia)

```
#### (a)

```{r}
library(ggplot2)
print(head(energia,3))
ggplot(energia, aes(Hora,Energía))+
geom_point()+
geom_smooth(method = "lm", se = FALSE, color = "blue") +

  labs(title = "Diagrama de dispersión", x = "Hora", y = "Energía") +
  theme_minimal()

```  
<br>
Por lo visto en el diagrama de dispersión no pareciera haber una relación lineal entre las variables. \

```{r}
library(lmtest)
linMod <- lm(Energía ~ Hora, data = energia)
plot(energia$Hora,energia$Energía,xlab="Hora",ylab="Energia",
     main="Energia vs Hora")

abline(linMod,col="darkviolet",lwd=2)
```

<br>

#### (b)

```{r}

summary(linMod)
```
```{r}
cor(energia$Hora, energia$Energía)^2
```

Para verificar la normalidad de los residuos hacemos uso del test de Shapiro Wilk.

```{r}
energias<- energia
energias$prediccion <- linMod$fitted.values
energias$residuos <- linMod$residuals

ggplot(data=energias, aes(x=residuos)) + 
  geom_histogram(aes(y=..density..)) + 
  labs(title="Histograma de los residuos") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

```
```{r}
qqnorm(linMod$residuals)
qqline(linMod$residuals)
``` 
```{r}
shapiro.test(linMod$residuals)
```
El test de Shapiro Wilk arroja un p-value de 0.006752, por lo que se rechaza la hipótesis nula de normalidad de los residuos. \



#### (c) 
<br>
Como se rechazó la hipótesis nula de normalidad de los residuos, se procede a realizar una transformación de las variables.

```{r}
#Transformación de Box & Cox
# Puedo aplicar Box y Cox porque la variable respuesta Y Energía es positiva.
library(MASS)

bc <- boxcox(Energía ~ Hora, lambda = -4:2, data = energia)

lambda_optimo <- bc$x[which.max(bc$y)]
lambda_optimo

```   
Se pudo aplicar el Box y Cox porque las variable respuesta tomaba valores positivos. Observando el gráfico se ve que el lambda óptimo es de -1.3 aproximadamente. \

Como la forma de estras transformaciones es 

$$
L(y) =
\begin{cases}
\displaystyle \frac{y^{\lambda} - 1}{\lambda}, & \text{si } \lambda \ne 0 \\
\ln(y), & \text{si } \lambda = 0
\end{cases}
$$
en este caso $\lambda \ne 0$, por lo tanto la transformación es:

$$L(y)=\frac{y^{-1.33}-1}{-1.33}$$
```{r, echo= TRUE}
energia$lambda_optimo <- (energia$Energía^lambda_optimo -1)/lambda_optimo

modelo_transformado <- lm(lambda_optimo ~ Hora, data = energia)

summary(modelo_transformado)
```
#### (d)
```{r}
library(lmtest)

plot(energia$Hora,energia$lambda_optimo,xlab="Hora",ylab="Energia transformada",
     main="Energia Transformada vs Hora")

abline(modelo_transformado,col="darkviolet",lwd=2)
```
```{r, echo=TRUE}
#Análisis diagnóstico del modelo transformado
shapiro.test(modelo_transformado$residuals)
```
Test de Breusch Pagan para la homocedasticidad:
```{r}
bptest(modelo_transformado) 

```
```{r}
dwt(modelo_transformado)
```


Intervalos de confianza y de predicción para el modelo original

```{r}
#Bandas de predicción
predichos <- predict (object=linMod, interval= "prediction", level=0.95)
nuevos_datos <- data.frame(energia, predichos)

ggplot(nuevos_datos, aes(x= Hora, y=Energía)) +
  geom_point(col= "#0B3B2E") +
geom_line(aes(y=lwr), color="#088A4B", linetype="dashed" ) +
geom_line(aes(y=upr), color="#088A4B" , linetype="dashed" ) +
geom_smooth(method=lm, formula=y ~ x, se=TRUE, level=0.95,
col="#088A4B", fill="pink2") +theme_light()

```
```{r}
#Bandas de predicción
predichos2 <- predict (object=modelo_transformado, interval= "prediction", level=0.95)
nuevos_datos2 <- data.frame(energia, predichos2)

ggplot(nuevos_datos2, aes(x= Hora, y=Energía)) +
  geom_point(col= "#0B3B2E") +
geom_line(aes(y=lwr), color="blue", linetype="dashed" ) +
geom_line(aes(y=upr), color="blue" , linetype="dashed" ) +
geom_smooth(method=lm, formula=y ~ x, se=TRUE, level=0.95,
col="#088A4B", fill="pink2") +theme_light()
```

```{r, echo=TRUE}
#Analizamos ambos tipos de intervalos en todos los modelos probados cuando horas=27.5

IC<-predict(linMod,newdata=data.frame(Hora= 27.5),interval="confidence")
IP<-predict(linMod,newdata=data.frame(Hora=27.5),interval="prediction")

print(IC)  

```
```{r}
print(IP)
```
```{r}
#Intervalos de confianza y de predicción para el modelo transformado
IC2<-predict(modelo_transformado,newdata=data.frame(Hora=27.5),interval="confidence")
IP2<-predict(modelo_transformado,newdata=data.frame(Hora=27.5),interval="prediction")

IC2
```
```{r}
IP2
```
```{r}
L_y<- IC2[1,1]
lambda<- lambda_optimo
y= (lambda*L_y+1)^(1/lambda)  
y
```

```{r}
plot(energia$Hora,energia$Energía, xlab="Hora",ylab="Energía",
     main="Hora vs Energía",xlim=c(20,30))

abline(linMod,col="darkviolet",lwd=2)
abline(modelo_transformado,col="pink",lwd=2)
abline(h=y,col="darkblue",lwd=2)
abline(v=27.5,col="darkblue",lwd=2)
```

## <span style="color:#18bc9c"> Tratamiento de la heterocedasticidad</span>

### Ejercicio 1.4

<br>
Se obtuvieron datos históricos del mercado inmobiliario de una
ciudad de Nueva Taipei, en Taiwan. La base es inmobiliaria.xlsx .\
<br>
Las características son:<br>
edad: Edad de la propiedad (en años).<br>
distancia: La distancia a la estación de transporte más cercana (en metros).<br> negocios: Cantidad de negocios de conveniencia en las cercanías a
una distancia realizable a pie.<br>
latitud: Latitud de la ubicación de la propiedad (en grados).<br>
longitud: Longitud de la ubicación de la propiedad (en grados).<br>
precio: Precio por metro cuadrado (en miles de dólares).<br>
<br>
Se quiere investigar si el precio de las propiedades puede ser estimado en
función de alguna de las variables disponibles.\
<br>
(a) Analizar si el precio depende de alguna de las variables.\
<br>
(b) Estudiar la linealidad de la relación precio-distancia.\
<br>
(c) Estimar los coeficientes del modelo y realizar el análisis diagnóstico de
los residuos del mismo. Utilizar para este análisis los gráficos de residuos
versus valores ajustados, el qq-plot de los residuos, la grafica de residuos
versus leverage.\
<br>
(d) Aplicar los test de Durbin-Watson Breush-Pagan.\
<br>
(e) Analice la presencia de outlier y verifique si coinciden con los puntos
influyentes.\
<br>
<br>

#### (a)

```{r}
inmobiliaria <- read.csv("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/inmobiliaria.csv", sep=";")
print(dim(inmobiliaria))
head(inmobiliaria,3)
```
```{r}
par(bg="white")
pairs(inmobiliaria) 
```

Mirando los gráficos podría decirse que el precio tiene relación con la distancia, los negocios y la edad. 

#### (b)



```{r, echo=TRUE}
#Analizamos la normalidad de las variables.

par(mfrow = c(1, 2))
hist(inmobiliaria$precio, breaks = 10, main = "", xlab = "Precio", border = "#2c3e50")
hist(inmobiliaria$distancia, breaks = 10, main = "", xlab = "Distancia", border = "#18bc9c")
```

```{r, echo=TRUE}
qqnorm(inmobiliaria$precio, main = "Precio", col = "#2c3e50")
qqline(inmobiliaria$precio)
qqnorm(inmobiliaria$distancia, main = "Distancia", col = "#18bc9c")
qqline(inmobiliaria$distancia)
```
```{r}

print(shapiro.test(inmobiliaria$precio))

print( shapiro.test(inmobiliaria$distancia))

``` 
Análisis de normalidad multivariada - Test de Henze Zirkler

```{r}
library(MVN)
attach(inmobiliaria)
precio_distancia=data.frame(precio,distancia)
#Usamos Test Henze-Zirkler para evaluar normalidad multivariada (bivariada en este caso)
respuesta_testHZ<-mvn(precio_distancia , mvnTest = "hz")
respuesta_testHZ
```
```{r}
respuesta_testHZ$multivariateNormality
```

Cálculo de correlación

```{r, echo=TRUE}
cor(inmobiliaria$precio, inmobiliaria$distancia, method= "pearson")
```  
```{r}
cor.test(inmobiliaria$precio, inmobiliaria$distancia, method= "pearson")
```
Modelo Lineal

```{r}
modelo_distancia <- lm(precio ~ distancia, data = inmobiliaria)
summary(modelo_distancia)
```
Análisis de la normalidad de los residuos - Test Shapiro Wilk

```{r}
inmobiliaria$prediccion <- modelo_distancia$fitted.values
inmobiliaria$residuos <- modelo_distancia$residuals

ggplot(data= inmobiliaria, aes(x=residuos)) + 
  geom_histogram(aes(y=..density..)) + 
  labs(title="Histograma de los residuos") + 
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))

```


```{r}
qqnorm(modelo_distancia$residuals)
qqline(modelo_distancia$residuals)
```
```{r}
shapiro.test(modelo_distancia$residuals)
```
El p-value es menor a 0.05, por lo que se rechaza la hipótesis nula de normalidad de los residuos. \
<br>

#### (c)

# Ejercicios ppt

```{r}
damascos <- read_excel("~/AUSTRAL/RegresionAvanzada/Archivos de datos para TP-20250407/Datos para TPs-20230606/damascos.xlsx")
head(damascos,3)
```
```{r}

mod_hojas <-lm (PESOF~LONGF, data=damascos)
print(summary(mod_hojas))
anova(mod_hojas)
``` 

```{r}

predichos <- predict(object=mod_hojas, interval= "prediction", level=0.95)
nuevos_datos <- data.frame (damascos, predichos)

ggplot(nuevos_datos, aes(x= LONGF, y= PESOF)) +
  geom_point(col= "#0B3B2E") +
  geom_line(aes(y=lwr), color="#088A4B", linetype="dashed" ) +
  geom_line(aes(y=upr), color="#088A4B" , linetype="dashed" ) +
  geom_smooth(method=lm, formula=y ~ x, se=TRUE, level=0.95,
              col="#088A4B", fill="pink2") +theme_light()
```
```{r}
library(nortest)

residuos= residuals(mod_hojas)
residuos_df = data.frame(residuos = residuos)
print( shapiro.test(residuos))
print(ad.test (residuos))
print(lillie.test(residuos))
```
```{r}
library(car)
qqPlot(residuos, pch=19, col="#2c3e50", main="QQ-plot de los residuos del modelo lineal",
xlab= "Cuantiles teóricos",
ylab= "Cuantiles muestrales")
```
```{r, echo= TRUE}
#Visualización de los residuos

residuos_df = data.frame(indice = 1:length(residuos), residuos = residuos)

ggplot(residuos_df, aes(x = indice, y = residuos)) +
  geom_point(color = "#2c3e50", size = 2) +
  labs(title = "Gráfico de residuos",
       x = "Índice",
       y = "Residuo") +
  theme_minimal()
```
```{r}
library(lmtest)
dwtest(mod_hojas, alternative= "two.sided", iterations=1000)
```
```{r}
#homocedasticidad
print( bptest(mod_hojas))

print(gqtest(mod_hojas))
```
Los dos test de homocedasticidad arrojan que no se puede rechazar la hipótesis nula de homocedasticidad. \
<br> 
Validación gráfica de la homocedasticidad. 

```{r}
# se utiliza el gráfico de residuales vs variable predictora. Alerta: estructura de embudo

